{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os\n",
        "import re\n",
        "import scipy"
      ],
      "metadata": {
        "id": "jPzHCBpoRlRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ2pg0arGD1g",
        "outputId": "b1706a72-e974-457e-ac53-691532e6a7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD2Wd0fGaozA"
      },
      "outputs": [],
      "source": [
        "# Extract the digits from the filename\n",
        "def extract_digits(string):\n",
        "    return ''.join(re.findall(r'\\d+', string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjiOBDu9cyAb"
      },
      "outputs": [],
      "source": [
        "# Apply hair segmentation mask to original image\n",
        "def load_and_preprocess(img_path, mask_path):\n",
        "\n",
        "  # Read image from path\n",
        "  img = cv2.imread(img_path)\n",
        "\n",
        "  # Read hair segmentation mask from path\n",
        "  mask = cv2.imread(mask_path)\n",
        "\n",
        "  # Resize image to match the size of the mask\n",
        "  img = cv2.resize(img, (mask.shape[1], mask.shape[0]))\n",
        "\n",
        "  # Convert color space from BGR to HSV\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  #img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) #UNCOMMENT THIS LINE TO GENERATE masks on the RGB color space!\n",
        "\n",
        "  # Filter out the hair pixels from the rest of the image\n",
        "  hair_mask = cv2.bitwise_and(img, mask)\n",
        "\n",
        "  return hair_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8BeYdesf5jL"
      },
      "outputs": [],
      "source": [
        "def generate_mask(hair_mask):\n",
        "\n",
        "  # Store dimensions of input image\n",
        "  height, width, _ = hair_mask.shape\n",
        "\n",
        "  # Generate the x and y coordinates of every pixel in the input image\n",
        "  x_coord, y_coord = np.meshgrid(np.arange(width), np.arange(height))\n",
        "\n",
        "  # Stack the x and y coordinates on top of each to create a N x M x 2 tensor\n",
        "  coords = np.stack((x_coord, y_coord), axis=2)\n",
        "\n",
        "  # Concatenate the x and y coordinates to the 'channel' dimension after the color channels (ex. N x M x 3 --> N x M x 5)\n",
        "  hair_mask = np.concatenate((hair_mask, coords), axis=2)\n",
        "  #print(f\"The dimensions of the image are {img.shape}\")\n",
        "\n",
        "  # Extract the hair pixels\n",
        "  hair_pixels = hair_mask[~np.all(hair_mask[:,:,0:3] == 0, axis=2)]\n",
        "  #print(hair_pixels.shape)\n",
        "\n",
        "  # Fit GMM with 2 clusters using the 'Hue' and 'Saturation' dimensions of the HSV color model\n",
        "  kmeans = KMeans(n_clusters=2).fit(hair_pixels[:,[0,1,2]])                                          # UNCOMMENT THIS LINE to use the KMEANS model (+ the related 'cluster_pixels' line below)\n",
        "  #gm = GaussianMixture(n_components=2).fit_predict(hair_pixels[:,[0,1,2]])                            # NOTICE: [0,1] to [0,1,2] if using the RGB color space! (In the HSV color space, only the first two channels are currently being used)\n",
        "\n",
        "  # Add column indicating cluster assignments to \"hair_pixels\" matrix\n",
        "  cluster_pixels = np.concatenate((hair_pixels, kmeans.labels_[:, np.newaxis]), axis=1)            # UNCOMMENT THIS LINE to use the KMEANS model (+ the related 'KMeans' line above)\n",
        "  #cluster_pixels = np.concatenate((hair_pixels, gm[:, np.newaxis]), axis=1)\n",
        "\n",
        "  # Extract the pixels belonging to each cluster\n",
        "  cluster_0 = cluster_pixels[cluster_pixels[:,5] == 0]\n",
        "  cluster_1 = cluster_pixels[cluster_pixels[:,5] == 1]\n",
        "\n",
        "  # Assign the pixels in the smaller cluster to a new variable\n",
        "  min_cluster = cluster_0[:,0:5] if cluster_0.shape[0] < cluster_1.shape[0] else cluster_1[:,0:5]\n",
        "\n",
        "  # Assign the pixels in the larger cluster to a new variable\n",
        "  maj_cluster = cluster_0[:,0:5] if cluster_0.shape[0] > cluster_1.shape[0] else cluster_1[:,0:5]\n",
        "  #print(min_cluster.shape)\n",
        "\n",
        "  # Create the black pixel 'canvas' for the segmentation mask\n",
        "  min_color_mask = np.zeros((height, width, 3))\n",
        "  maj_color_mask = np.zeros((height, width, 3))\n",
        "\n",
        "  # Map the pixels in the smaller cluster onto the 'canvas'\n",
        "  for i, coords in enumerate(min_cluster[:, 3:]):\n",
        "    coords[0], coords[1] = coords[1], coords[0]\n",
        "    min_color_mask[tuple(coords)] = [255,255,255]\n",
        "\n",
        "  for i, coords in enumerate(maj_cluster[:, 3:]):\n",
        "    coords[0], coords[1] = coords[1], coords[0]\n",
        "    maj_color_mask[tuple(coords)] = [255,255,255]\n",
        "\n",
        "  return min_color_mask, maj_color_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDF1eqIAsQyn"
      },
      "outputs": [],
      "source": [
        "def fill_mask(min_color_mask, thickness):\n",
        "  gray = cv2.cvtColor(min_color_mask.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "  ret, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
        "  contours, hierarchy = cv2.findContours(binary, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
        "  min_color_mask = cv2.drawContours(min_color_mask, contours, -1, (255, 255, 255), thickness=thickness)\n",
        "  return cv2.fillPoly(min_color_mask, pts=contours, color=(255, 255, 255))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBoIlw_DWdK0",
        "outputId": "8e0a3c9f-f24b-4692-9db5-4cd9a302b7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66 66 66 32\n"
          ]
        }
      ],
      "source": [
        "image_path = '/content/drive/MyDrive/Touch Up The Roots/Images'\n",
        "mediapipe_path = '/content/drive/MyDrive/Touch Up The Roots/Hair Segmentation/MediaPipe/Output'\n",
        "selfie_multiclass_path = '/content/drive/MyDrive/Touch Up The Roots/Hair Segmentation/SelfieMulticlass/Output'\n",
        "clipseg_path = '/content/drive/MyDrive/Touch Up The Roots/Hair Segmentation/CLIPSeg/Output'\n",
        "\n",
        "images = os.listdir(image_path)\n",
        "mediapipe_masks = os.listdir(mediapipe_path)\n",
        "selfie_multiclass_masks = os.listdir(selfie_multiclass_path)\n",
        "clipseg_masks = os.listdir(clipseg_path)\n",
        "\n",
        "print(len(images), len(mediapipe_masks), len(selfie_multiclass_masks), len(clipseg_masks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tbxXxa7h5KX"
      },
      "outputs": [],
      "source": [
        "# Sort image and mask folders so that images and masks are matched up\n",
        "image_tuples = []\n",
        "mediapipe_tuples = []\n",
        "selfie_tuples = []\n",
        "clipseg_tuples = []\n",
        "for i in range(len(images)):\n",
        "  image_tuples.append((int(extract_digits(images[i])), images[i]))\n",
        "  mediapipe_tuples.append((int(extract_digits(mediapipe_masks[i])), mediapipe_masks[i]))\n",
        "  selfie_tuples.append((int(extract_digits(selfie_multiclass_masks[i])), selfie_multiclass_masks[i]))\n",
        "\n",
        "image_tuples = sorted(image_tuples, key=lambda x: x[0])\n",
        "mediapipe_tuples = sorted(mediapipe_tuples, key=lambda x: x[0])\n",
        "selfie_tuples = sorted(selfie_tuples, key=lambda x: x[0])\n",
        "\n",
        "images = list(map(lambda x: x[1], image_tuples))\n",
        "mediapipe_masks = list(map(lambda x: x[1], mediapipe_tuples))\n",
        "selfie_multiclass_masks = list(map(lambda x: x[1], selfie_tuples))\n",
        "\n",
        "for i in range(len(clipseg_masks)): #images\n",
        "  clipseg_tuples.append((int(extract_digits(clipseg_masks[i])), clipseg_masks[i]))\n",
        "clipseg_tuples = sorted(clipseg_tuples, key=lambda x: x[0])\n",
        "clipseg_masks = list(map(lambda x: x[1], clipseg_tuples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwha3A55R5qa"
      },
      "outputs": [],
      "source": [
        "# mediapipe_output_path = '/content/drive/MyDrive/Touch Up The Roots/Minority Color Hair Segmentation/KMeans on RGB/Expanded Mask (Mediapipe)'\n",
        "\n",
        "# # Generate minority color segmentation masks from MediaPipe hair segmentation masks\n",
        "# for i in range(len(images)):\n",
        "#   img_path = os.path.join(image_path, images[i])\n",
        "#   mask_path = os.path.join(mediapipe_path, mediapipe_masks[i])\n",
        "#   hair_mask = load_and_preprocess(img_path, mask_path)\n",
        "#   min_color_mask = generate_mask(hair_mask)\n",
        "#   min_color_mask = fill_mask(min_color_mask, thickness=5)\n",
        "\n",
        "#   # Write image to output folder\n",
        "#   img_num = extract_digits(images[i])\n",
        "#   cv2.imwrite(f'{mediapipe_output_path}/min_color_mask{img_num}.jpg', min_color_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrf8ZZtYNz5"
      },
      "outputs": [],
      "source": [
        "# selfie_multiclass_output_path = '/content/drive/MyDrive/Touch Up The Roots/Minority Color Hair Segmentation/KMeans on RGB/Expanded Mask (SelfieMulticlass)'\n",
        "\n",
        "# # Generate minority color segmentation masks from SelfieMulticlass hair segmentation masks\n",
        "# for i in range(len(images)):\n",
        "#   img_path = os.path.join(image_path, images[i])\n",
        "#   mask_path = os.path.join(selfie_multiclass_path, selfie_multiclass_masks[i])\n",
        "#   hair_mask = load_and_preprocess(img_path, mask_path)\n",
        "#   min_color_mask = generate_mask(hair_mask)\n",
        "#   min_color_mask = fill_mask(min_color_mask, thickness=5)\n",
        "\n",
        "#   # Write image to output folder\n",
        "#   img_num = extract_digits(images[i])\n",
        "#   cv2.imwrite(f'{selfie_multiclass_output_path}/min_color_mask{img_num}.jpg', min_color_mask)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clipseg_images = []\n",
        "clipseg_digits = list(map(lambda x: x[0], clipseg_tuples))\n",
        "for i in images:\n",
        "  if int(extract_digits(i)) in clipseg_digits:\n",
        "    clipseg_images.append(i)\n",
        "len(clipseg_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYqimoSaKn68",
        "outputId": "4be1f8ab-4f31-4400-ede6-82aafe05e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REDO '/content/drive/MyDrive/Touch Up The Roots/Minority Color Hair Segmentation/KMeans on RGB/Expanded Mask (SelfieMulticlass)'\n",
        "clipseg_output_path = '/content/drive/MyDrive/Touch Up The Roots/Minority Color Hair Segmentation/KMeans on HSV/Expanded Mask (CLIPSeg)'\n",
        "# Generate minority color segmentation masks from SelfieMulticlass hair segmentation masks\n",
        "for i in range(len(clipseg_images)):\n",
        "  img_path = os.path.join(image_path, clipseg_images[i])\n",
        "  mask_path = os.path.join(clipseg_path, clipseg_masks[i])\n",
        "  hair_mask = load_and_preprocess(img_path, mask_path)\n",
        "  min_color_mask = generate_mask(hair_mask)\n",
        "  min_color_mask = fill_mask(min_color_mask, thickness=5)\n",
        "\n",
        "  # Write image to output folder\n",
        "  img_num = extract_digits(clipseg_images[i])\n",
        "  cv2.imwrite(f'{clipseg_output_path}/min_color_mask{img_num}.jpg', min_color_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "NoFXRlf65X7t",
        "outputId": "600b4c62-2bc9-4d1a-8017-541453354aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'astype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-98af0b6557ef>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mhair_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmin_color_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhair_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmin_color_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_color_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Write image to output folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b32cad4bdf67>\u001b[0m in \u001b[0;36mfill_mask\u001b[0;34m(min_color_mask, thickness)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfill_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_color_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_color_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmin_color_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_color_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthickness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'astype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of test image\n",
        "test_image_path = os.path.join(image_path, images[0])\n",
        "\n",
        "# Outputs colored hair segmentation mask\n",
        "test_mask = load_and_preprocess(test_image_path, os.path.join(mediapipe_path, mediapipe_masks[0]))\n",
        "\n",
        "# Generate gray hair segmentation mask\n",
        "gray = cv2.cvtColor(load_and_preprocess(test_image_path, os.path.join(mediapipe_path, mediapipe_masks[0])), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Generate min and maj color masks\n",
        "min, maj = generate_mask(test_mask)"
      ],
      "metadata": {
        "id": "no6kQKLyJ6uY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142b4499-8dd8-45ae-a28f-e09e49a3c6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colored_min = np.zeros_like(test_mask)\n",
        "colored_min[np.where(min > 0)] = test_mask[np.where(min > 0)]\n",
        "\n",
        "colored_maj = np.zeros_like(test_mask)\n",
        "colored_maj[np.where(maj > 0)] = test_mask[np.where(maj > 0)]"
      ],
      "metadata": {
        "id": "IBMP67aUbg8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(image, min_color_mask, maj_color_mask):\n",
        "  \"\"\"\n",
        "  This function computes the Wasserstein distance between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n",
        "\n",
        "  In theory, having the touched up region's pixel color distribution appear as similar as possible to that of the majority color region is desirable because\n",
        "  the touched up region will blend in better with the remainder of the hair. This improves the color while preserving more \"naturalness\".  Natural hair texture is often lost because it\n",
        "  appears that the hair texture in an image is numerically represented by particular patterns of minor variations in RGB values (the RGB values of which are approximated by grayscale pixel intensity)\n",
        "  and that these patters are vulnerable to being \"smoothed\" out of existence by the Stable Diffusion model.  Attempting to optimize this loss function is attempting to minimize the amount of\n",
        "  \"smoothness\" afflicting the touched up region while pushing the mean (and median) pixel color in the direction of the majority color region.\n",
        "\n",
        "\n",
        "  Parameters:\n",
        "  image (numpy array): The original image.\n",
        "  min_color_mask (numpy array): The minority color segmentation mask.\n",
        "  maj_color_mask (numpy array): The majority color segmentation mask.\n",
        "\n",
        "  Returns:\n",
        "  loss (float): The Wasserstein distance between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create black background image equal in size to the original image\n",
        "  colored_min_region = np.zeros_like(image)\n",
        "  colored_maj_region = np.zeros_like(image)\n",
        "\n",
        "  # Extract the indices of the pixel regions of each mask\n",
        "  min_mask_regions = np.where(min_color_mask > 0)\n",
        "  maj_mask_regions = np.where(maj_color_mask > 0)\n",
        "\n",
        "  # Map the colored pixels from the minority and majority color segmentation masks onto the black background image\n",
        "  colored_min_region[min_mask_regions] = image[min_mask_regions]\n",
        "  colored_maj_region[maj_mask_regions] = image[maj_mask_regions]\n",
        "\n",
        "  \"\"\"\n",
        "  The two lines below convert RGB images to grayscale images.\n",
        "  NOTE: The formula to do, based on relative perception of color brightness, is the following: 0.299 ∙ Red + 0.587 ∙ Green + 0.114 ∙ Blue\n",
        "  The corresponding grayscale pixel value is a weighted sum of the RGB pixel values and represents the pixel \"intensity\".\n",
        "  \"\"\"\n",
        "  gray_min_region = cv2.cvtColor(colored_min_region, cv2.COLOR_BGR2GRAY)\n",
        "  gray_maj_region = cv2.cvtColor(colored_maj_region, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Flatten each greyscale image to a vector\n",
        "  gray_min_vector = gray_min_region.flatten()\n",
        "  gray_maj_vector = gray_maj_region.flatten()\n",
        "\n",
        "  # Extract nonzero pixel values (ie discard the black background pixels)\n",
        "  gray_min_pixels = gray_min_vector[gray_min_vector > 0]\n",
        "  gray_maj_pixels = gray_maj_vector[gray_maj_vector > 0]\n",
        "\n",
        "  # Compute histogram of pixel intensities for the minority color segmentation mask\n",
        "  min_mask_distribution = np.bincount(gray_min_pixels) / gray_min_pixels.size\n",
        "\n",
        "  # Compute histogram of pixel intensities for the majority color segmentation mask\n",
        "  maj_mask_distribution = np.bincount(gray_maj_pixels) / gray_maj_pixels.size\n",
        "\n",
        "  # Ensure that the probability distribtion sums to 1\n",
        "  assert(min_mask_distribution.sum() == 1)\n",
        "  assert(maj_mask_distribution.sum() == 1)\n",
        "\n",
        "  # Compute Wasserstein distance between the two histograms of pixel intensities\n",
        "  distribution_distance = scipy.stats.wasserstein_distance(min_mask_distribution, maj_mask_distribution, u_weights=None, v_weights=None)\n",
        "  loss = distribution_distance\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "jBqEVVFKcxWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss(test_mask, min, maj)\n",
        "\n",
        "# visualize histogram of empirical counts?"
      ],
      "metadata": {
        "id": "VmUlcTNtWDDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3e9794-245b-4727-a5b4-354b1853dd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003999500062492188"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1haJW78sAaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5NgBih25CU",
        "outputId": "06d8cb17-6ad1-422f-bf12-574c3b2df2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uI3Juw8c8zh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}